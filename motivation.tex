\section{Motivation and Background}\label{sec-motivation}

In this section, we present some background information on 
factors that shaped the design and deployment of 
Sensibility Testbed. First, we offer the motivations behind its 
development. 
%the desire to balance the benefits of such research 
%with the very real privacy risks that could potentially occur if a 
%device owner allows access to his/her device. 
Next, we look at one solution to the problem of collecting approximate 
data in place of real data. Finally, we talk about 
the traditional role of IRB in research involving human subjects 
and to data accessed remotely. 

\textbf{Motivation: the potential and the risks of accessing sensor data.}
Having access to data from the enormous number of smartphones 
in use today could be tremendously valuable to researchers in a 
number of fields, ranging from health and fitness to the social sciences. 
%One study suggested that collecting sensor-based data is vastly 
%superior to the use of surveys, which can suffer from incomplete 
%content, biases, and a lack of continuity. Because survey-based 
%studies also are "fixed in time," there is no way to monitor behavior 
%over extended periods of time. 
It is also a research method that does not incur high maintenance costs. 
Unfortunately, it is a research method that is ``expensive'' in other ways, 
primarily time. Recruitment of volunteers is time-consuming and often 
frustrating. %There are a number of reasons why this recruitment is so 
%difficult. One is that 
First, there is no easy method to build a pool of willing participants, 
so it needs to be re-done every time a new experiment is proposed. 
Second, there are complications due to institutional policies on work 
with human subjects (discussed later in this section) that affect who 
can be recruited and under what conditions. An individual recruiting 
on her own is likely to end up with a more homogenous set of subjects, 
drawn either from one locality or interest group. Last but not least, there 
is a very real threat of damage to a participant's device and invasion 
of the participant's privacy. Participants face a significant risk if 
they offer their device to a researcher.

%Due to recent privacy breaches and security break-ins to mobile systems, 
%device security and personal privacy are genuinely at risk when a person 
%uses a smartphone or tablet \cite{breach}. 
%%Apps can post tweets to a 
%%user's Twitter account without asking for permission~\cite{tweet}. 
%A calculator app might send the user's location to an advertisement 
%server~\cite{calc}. Sensor data from the accelerometer 
%or gyroscope can be sufficient to infer the locations of touch-screen 
%taps, and thus infer a user's password~\cite{cai2011touchlogger}.
%Compromised apps can even let criminals break into an individual's 
%bank account~\cite{starbucks}. 
%%As a result, device owners 
%%are aware that running apps on their smartphones can raise privacy 
%%and security risks. 
%A major reason for the prevelant privacy breaches is that on many mobile 
%systems, such as Android, 
%only a sub-set of sensors like GPS and bluetooth are considered risky, 
%and their access is mediated~\cite{android-sec}. Other sensors 
%such as accelerometer, gyroscope, etc., 
%%are considered to be innocuous, 
%require no permission to access. Furthermore, %research shows that 
%device owners are often oblivious to the implications of granting access to
%a particular type of sensor or resource~\cite{felt2012android}. It is 
%therefore challenging to conduct research on end-user devices
%in compliance with ethical standards~\cite{zevenbergen2013ethical}.

%However, having access to data from the enormous number of smartphones 
%in use today could be tremendously valuable to the research 
%community. As these devices belong to ordinary people, conducting
%research on these devices does not incur high maintenance costs. 
%Accelerometers on end-user devices could detect vibrations within 
%the frequency and intensity range of seismic waves, and assist 
%distributed earthquake detection~\cite{faulkner2011next}. GPS, 
%WiFi, and cellular triangulation can be employed in distributed 
%networks of sensors for traffic monitoring and accident 
%prevention~\cite{mohan2008nericell, thiagarajan2009vtrack}. 
%For the research community, accessing this data depends on its 
%ability to provide strong protection to device owners from privacy 
%and security breaches. In this work, we try to address two issues. 

Solutions to the privacy and security problem have been difficult to 
identify, in part because the threat is not broadly recognized by device 
owners. Device owners may not fully 
understand the implications of granting access to a particular type of 
sensor or resource~\cite{felt2012android}. It is therefore challenging to 
conduct research on end-user devices in compliance with ethical standards, 
without an organized approach to enforcing those 
standards~\cite{zevenbergen2013ethical}. Another reason for 
the prevalent privacy breaches on many mobile systems %such as Android and iOS, 
is that only a sub-set of sensors like GPS and Bluetooth are considered risky, 	
and have their access mediated~\cite{android-sec}. Other sensors 
such as accelerometer, gyroscope, etc., require no permission to access, 
thus leaving them open to attack. A first step to expanding the use of sensors on mobile devices is to design solutions that address the security needs of device owners and the practical needs of potential researchers.

\textbf{Restricted data access as privacy protection.}
%Despite these risks of using sensors, 
How specific does data need to be in order to be useful? Studies have indicated that 
sensor data can be accessed without compromising device 
owners' privacy or sacrificing service functioning if that data is generalized. 
Such data that substitutes approximate sensor values does not 
directly violate the privacy of the device owner, and still provides 
valuable information to the researcher conducting the study.
For this reason, researchers have proposed 
substituting mocked~\cite{beresford2011mockdroid} or 
anonymized~\cite{zhou2011taming} data in place of real data. 
For example, in location-based services such as maps, 
restaurant guides, and bus schedules, end users can still use the 
service even if a device only provides a discretized 
location~\cite{amini2011cache, krumm2007inference}. The use of 
generalized data could also encourage more volunteer participation. 
Recent research shows that more than half of the 
surveyed individuals had no problem in supplying imprecise 
sensor data from their personal devices~\cite{fawaz2014location}. 
Most participants could accommodate some reduced application 
functionality, as long as their privacy was protected. Those 
applications included in the survey
ranged from location-based search (e.g., Yelp) and social 
network apps to gaming and weather forecasting apps. 
%the imprecise information is sufficient for a large class of services. \lois{these two sentences have to be clarified. How does one survey an application? And, why would participants be willing to a loss of functionality?}
%The US Federal Communications Commission requires 
%emergency rescue and 
%response teams to be able to estimate a 911 wireless emergency 
%caller's position with an accuracy of 125~m~\cite{gruteser2003anonymous, 
%reed1998overview}. \yanyan{too many examples?}

Based on these facts, restricting 
the amount of data accessible, such as reducing the precision or 
access frequency, offers an effective privacy protection mechanism to 
provide to end users. In this work, we coin the term \textit{data blurring}
as one privacy protection mechanism. However, it must be ensured that  
experiments do not collect more data than they need for providing 
their functionaly. 
An automatic way to substitute approximate data in place of explicit raw data 
is required to achieve realiable and easily usable blurring.


\textbf{IRB policies: guiding ethical behavior.}
%On the other hand, research institutions have also designed a 
%protocol based on the \textit{institutional review board (IRB)}, 
%to assess the ethics of a researcher's project, and review its methods. 
As mentioned earlier, every researcher's work is guided by an 
Institutional Review Board\footnote{\scriptsize Also known as an 
independent ethics committee (IEC), ethical review board (ERB), 
or research ethics board (REB).}, an internal group that serves as 
the ethical watch dogs for colleges, universities, government agencies, 
and other research institutions. It is the job of these boards, 
to approve, monitor, and review research involving human 
subjects~\cite{irb}. 
\begin{comment}
While a commitment to ethical treatment of humans who 
submit to experiments has always been part of the professional codes of most 
scientists, IRBs did not become ubiquitous in research facilities until the 
latter part of the 20th century.  Partly provoked by atrocities committed by 
the Nazis in the name of scientific experiments in the Second World War II, 
and partly inspired by directives from the medical community, including 
Declaration of Helsinki established In 1964 by the World Medical Association, 
research institutions formally acknowledged the need to protect human subjects 
in any research setting. Today, 
\end{comment}
IRBs require all researchers working under 
their aegis to submit the protocols of their studies in advance, with an aim to 
protect not only the physical and mental well-being of subjects, 
but also to protect any information about these individuals generated 
over the course of the study. However, enforcement of these policies becomes significantly harder when dealing with remote subjects. Although many current network 
testbeds require that researchers obtain IRB approval before conducting
an experiment on the testbed, these platforms do not provide a guarantee 
for IRB policy compliance~\cite{nandugudi2013phonelab, nikravesh2015mobilyzer}.
%In the case of PhoneLab, 
%it requires experimenters to obtain IRB approval. However, 
%it leaves it up to the experimenters to comply with their IRB policies in their 
%experiments~\cite{nandugudi2013phonelab, nikravesh2015mobilyzer}. 
%Similarly, Mobilyzer~\cite{nikravesh2015mobilyzer} provides a 
%measurement library that can be included in Android apps. 
%and requires explicit user consent. 
Therefore, there is no guarantee that an 
experiment will be compliant with a researcher's IRB policies. 
%promotes fully informed 
%consent and voluntary participation by prospective subjects. 

%IRB plays a central role in defining the policies
%appropriate for research at individual institutions. Experimenters
%first obtain an IRB approval at their institution. Then with these IRB
%policies, Sensibility Testbed, as an intermediate, codifies the data access 
%regulations and enforces them at the end-user mobile devices. 
%This is achieved through restricting data access  via
%a set of blurring layers. Each layer implements an IRB policy by substituting 
%approximate data in place of explicit, raw sensor data to the experiment code. Different layers 


%Our goal is to facilitate the enforcement of 
%IRB policies on behalf of researchers, and that experiments 
%do not collect more data than needed to provide their functionalities.
%This will also relieve researchers from the tedious work of 
%recruiting participants and enforcing IRB policies.

%In the domain of IRB, Alice and Bob are the participating subject, and 
%a researcher who conducts a research study on the subject, respectively.
%
%
%\textbf{Sensibility Testbed's default policies.}

